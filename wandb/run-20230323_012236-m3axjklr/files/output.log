test points sampled
15
ModuleList(
  (0): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (k): GConvSE3Partial(structure=[(8, 1)])
      (q): GConvSE3Partial(structure=[(8, 1)])
      (attn): GMABSE3(n_heads=8, structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
    )
    (cat): GCat(structure=[(8, 0), (9, 1), (8, 2), (8, 3)])
    (project): G1x1SE3(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
  )
  (1): GNormTFN()
  (2): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (k): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (q): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (attn): GMABSE3(n_heads=8, structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
    )
    (cat): GCat(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    (project): G1x1SE3(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
  )
  (3): GNormTFN()
  (4): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (k): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (q): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (attn): GMABSE3(n_heads=8, structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
    )
    (cat): GCat(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    (project): G1x1SE3(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
  )
  (5): GNormTFN()
  (6): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (k): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (q): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (attn): GMABSE3(n_heads=8, structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
    )
    (cat): GCat(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    (project): G1x1SE3(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
  )
  (7): GNormTFN()
  (8): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (k): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (q): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (attn): GMABSE3(n_heads=8, structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
    )
    (cat): GCat(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    (project): G1x1SE3(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
  )
  (9): GNormTFN()
  (10): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(64, 0)])
      (k): GConvSE3Partial(structure=[(64, 0)])
      (q): GConvSE3Partial(structure=[(64, 0)])
      (attn): GMABSE3(n_heads=8, structure=[(64, 0)])
    )
    (cat): GCat(structure=[(72, 0)])
    (project): AttentiveSelfInteractionSE3(in=[(72, 0)], out=[(64, 0)])
  )
)
Begin training
Scanobjectnn_4090_256_20_batch64_att_6_8
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[0|0] loss: 2.73835
training one epoch costs:26.35887384414673s
...[0|test] loss: 1.92893
Acc is {'acc': 0.3645833333333333}
Inference costs:2.2820847034454346s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[1|0] loss: 1.99602
training one epoch costs:25.1942036151886s
...[1|test] loss: 1.86145
Acc is {'acc': 0.3732638888888889}
Inference costs:2.314462423324585s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[2|0] loss: 2.04199
training one epoch costs:25.347646951675415s
...[2|test] loss: 1.90907
Acc is {'acc': 0.3611111111111111}
Inference costs:2.3138973712921143s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[3|0] loss: 1.86566
training one epoch costs:25.284790754318237s
...[3|test] loss: 1.91172
Acc is {'acc': 0.3159722222222222}
Inference costs:2.281728744506836s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[4|0] loss: 1.90273
training one epoch costs:25.26921820640564s
...[4|test] loss: 1.95077
Acc is {'acc': 0.3628472222222222}
Inference costs:2.2900924682617188s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[5|0] loss: 1.95071
training one epoch costs:25.166618585586548s
...[5|test] loss: 1.87735
Acc is {'acc': 0.3506944444444444}
Inference costs:2.3284199237823486s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[6|0] loss: 1.77977
training one epoch costs:25.156702041625977s
...[6|test] loss: 1.80611
Acc is {'acc': 0.3663194444444444}
Inference costs:2.3553667068481445s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[7|0] loss: 1.99744
training one epoch costs:25.05386471748352s
...[7|test] loss: 1.86985
Acc is {'acc': 0.3368055555555556}
Inference costs:2.2722959518432617s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[8|0] loss: 1.92790
training one epoch costs:25.063567399978638s
...[8|test] loss: 2.30858
Acc is {'acc': 0.25}
Inference costs:2.2934577465057373s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[9|0] loss: 2.30632
training one epoch costs:25.03416895866394s
...[9|test] loss: 2.63606
Acc is {'acc': 0.11805555555555555}
Inference costs:2.288222312927246s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[10|0] loss: 2.63218
Traceback (most recent call last):
  File "/home/sssak/EquivPerformer/pccls_run.py", line 244, in <module>
    main(FLAGS, UNPARSED_ARGV)
  File "/home/sssak/EquivPerformer/pccls_run.py", line 223, in main
    train_loss = train_epoch(epoch, model, task_loss, train_loader, optimizer, scheduler, FLAGS)
  File "/home/sssak/EquivPerformer/pccls_run.py", line 80, in train_epoch
    optimizer.step()
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/optim/adam.py", line 234, in step
    adam(params_with_grad,
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/optim/adam.py", line 300, in adam
    func(params,
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/optim/adam.py", line 363, in _single_tensor_adam
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
KeyboardInterrupt