test points sampled
15
ModuleList(
  (0): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (k): GConvSE3Partial(structure=[(8, 1)])
      (q): GConvSE3Partial(structure=[(8, 1)])
      (attn): GMABSE3(n_heads=8, structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
    )
    (cat): GCat(structure=[(8, 0), (9, 1), (8, 2), (8, 3)])
    (project): G1x1SE3(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
  )
  (1): GNormTFN()
  (2): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (k): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (q): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (attn): GMABSE3(n_heads=8, structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
    )
    (cat): GCat(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    (project): G1x1SE3(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
  )
  (3): GNormTFN()
  (4): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (k): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (q): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (attn): GMABSE3(n_heads=8, structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
    )
    (cat): GCat(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    (project): G1x1SE3(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
  )
  (5): GNormTFN()
  (6): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (k): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (q): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (attn): GMABSE3(n_heads=8, structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
    )
    (cat): GCat(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    (project): G1x1SE3(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
  )
  (7): GNormTFN()
  (8): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (k): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (q): GConvSE3Partial(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
      (attn): GMABSE3(n_heads=8, structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
    )
    (cat): GCat(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    (project): G1x1SE3(structure=[(8, 0), (8, 1), (8, 2), (8, 3)])
  )
  (9): GNormTFN()
  (10): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(64, 0)])
      (k): GConvSE3Partial(structure=[(64, 0)])
      (q): GConvSE3Partial(structure=[(64, 0)])
      (attn): GMABSE3(n_heads=8, structure=[(64, 0)])
    )
    (cat): GCat(structure=[(72, 0)])
    (project): AttentiveSelfInteractionSE3(in=[(72, 0)], out=[(64, 0)])
  )
)
Begin training
Scanobjectnn_4090_256_20_batch64_att_6_8
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[0|0] loss: 2.69633
training one epoch costs:25.680205821990967s
...[0|test] loss: 1.86309
Acc is {'acc': 0.3836805555555556}
Inference costs:2.2881433963775635s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[1|0] loss: 1.76183
training one epoch costs:24.36583399772644s
...[1|test] loss: 1.73605
Acc is {'acc': 0.3923611111111111}
Inference costs:2.323873281478882s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[2|0] loss: 1.75223
training one epoch costs:24.351429224014282s
...[2|test] loss: 1.45757
Acc is {'acc': 0.5173611111111112}
Inference costs:2.291414737701416s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[3|0] loss: 1.76528
training one epoch costs:24.383383750915527s
...[3|test] loss: 1.40987
Acc is {'acc': 0.5434027777777778}
Inference costs:2.2793805599212646s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[4|0] loss: 1.51620
training one epoch costs:24.55562114715576s
...[4|test] loss: 1.26913
Acc is {'acc': 0.5868055555555556}
Inference costs:2.327097177505493s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[5|0] loss: 1.25656
training one epoch costs:24.56258487701416s
...[5|test] loss: 1.31695
Acc is {'acc': 0.5659722222222222}
Inference costs:2.3157145977020264s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[6|0] loss: 1.38816
training one epoch costs:24.416411638259888s
...[6|test] loss: 1.21503
Acc is {'acc': 0.6076388888888888}
Inference costs:2.3179025650024414s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[7|0] loss: 1.00850
training one epoch costs:24.581031799316406s
...[7|test] loss: 1.21068
Acc is {'acc': 0.6197916666666666}
Inference costs:2.2994871139526367s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[8|0] loss: 0.91054
training one epoch costs:24.455108404159546s
...[8|test] loss: 1.25590
Acc is {'acc': 0.6006944444444444}
Inference costs:2.286186933517456s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[9|0] loss: 1.02327
training one epoch costs:24.4259090423584s
...[9|test] loss: 1.24010
Acc is {'acc': 0.6059027777777778}
Inference costs:2.297197103500366s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[10|0] loss: 0.99231
training one epoch costs:24.520562887191772s
...[10|test] loss: 1.16702
Acc is {'acc': 0.640625}
Inference costs:2.3122751712799072s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[11|0] loss: 0.94713
training one epoch costs:24.553590536117554s
...[11|test] loss: 1.11202
Acc is {'acc': 0.6614583333333334}
Inference costs:2.3145945072174072s
Saved: models/Scanobjectnn_4090_256_20_batch64_att_6_8.pt
[12|0] loss: 0.96294
Traceback (most recent call last):
  File "/home/sssak/EquivPerformer/pccls_run.py", line 244, in <module>
    main(FLAGS, UNPARSED_ARGV)
  File "/home/sssak/EquivPerformer/pccls_run.py", line 223, in main
    train_loss = train_epoch(epoch, model, task_loss, train_loader, optimizer, scheduler, FLAGS)
  File "/home/sssak/EquivPerformer/pccls_run.py", line 72, in train_epoch
    loss.backward()
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt