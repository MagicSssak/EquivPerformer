test points sampled
15
ModuleList(
  (0): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (k): GConvSE3Partial(structure=[(16, 1)])
      (q): GConvSE3Partial(structure=[(16, 1)])
      (attn): GMABSE3(n_heads=8, structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    )
    (cat): GCat(structure=[(16, 0), (17, 1), (16, 2), (16, 3)])
    (project): G1x1SE3(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
  )
  (1): GNormTFN()
  (2): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (k): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (q): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (attn): GMABSE3(n_heads=8, structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    )
    (cat): GCat(structure=[(32, 0), (32, 1), (32, 2), (32, 3)])
    (project): G1x1SE3(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
  )
  (3): GNormTFN()
  (4): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (k): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (q): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (attn): GMABSE3(n_heads=8, structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    )
    (cat): GCat(structure=[(32, 0), (32, 1), (32, 2), (32, 3)])
    (project): G1x1SE3(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
  )
  (5): GNormTFN()
  (6): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (k): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (q): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (attn): GMABSE3(n_heads=8, structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    )
    (cat): GCat(structure=[(32, 0), (32, 1), (32, 2), (32, 3)])
    (project): G1x1SE3(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
  )
  (7): GNormTFN()
  (8): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (k): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (q): GConvSE3Partial(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
      (attn): GMABSE3(n_heads=8, structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
    )
    (cat): GCat(structure=[(32, 0), (32, 1), (32, 2), (32, 3)])
    (project): G1x1SE3(structure=[(16, 0), (16, 1), (16, 2), (16, 3)])
  )
  (9): GNormTFN()
  (10): GSE3Res(
    (GMAB): ModuleDict(
      (v): GConvSE3Partial(structure=[(64, 0)])
      (k): GConvSE3Partial(structure=[(64, 0)])
      (q): GConvSE3Partial(structure=[(64, 0)])
      (attn): GMABSE3(n_heads=8, structure=[(64, 0)])
    )
    (cat): GCat(structure=[(80, 0)])
    (project): AttentiveSelfInteractionSE3(in=[(80, 0)], out=[(64, 0)])
  )
)
Begin training
Scanobjectnn_4090_256_20_batch16_att_6_16
Saved: models/Scanobjectnn_4090_256_20_batch16_att_6_16.pt
[0|0] loss: 2.70071
[0|100] loss: 2.26065
training one epoch costs:67.34673929214478s
...[0|test] loss: 1.77221
Acc is {'acc': 0.4357638888888889}
Inference costs:7.00521183013916s
Saved: models/Scanobjectnn_4090_256_20_batch16_att_6_16.pt
[1|0] loss: 1.69230
[1|100] loss: 1.76843
training one epoch costs:66.32026696205139s
...[1|test] loss: 1.61640
Acc is {'acc': 0.4583333333333333}
Inference costs:7.022329092025757s
Saved: models/Scanobjectnn_4090_256_20_batch16_att_6_16.pt
[2|0] loss: 1.71423
[2|100] loss: 1.52468
training one epoch costs:66.38265609741211s
...[2|test] loss: 1.52258
Acc is {'acc': 0.4947916666666667}
Inference costs:7.006772994995117s
Saved: models/Scanobjectnn_4090_256_20_batch16_att_6_16.pt
[3|0] loss: 1.07262
[3|100] loss: 1.59468
training one epoch costs:66.37235975265503s
...[3|test] loss: 1.60921
Acc is {'acc': 0.4878472222222222}
Inference costs:7.093721389770508s
Saved: models/Scanobjectnn_4090_256_20_batch16_att_6_16.pt
[4|0] loss: 2.01238
Traceback (most recent call last):
  File "/home/sssak/EquivPerformer/pccls_run.py", line 244, in <module>
    main(FLAGS, UNPARSED_ARGV)
  File "/home/sssak/EquivPerformer/pccls_run.py", line 223, in main
    train_loss = train_epoch(epoch, model, task_loss, train_loader, optimizer, scheduler, FLAGS)
  File "/home/sssak/EquivPerformer/pccls_run.py", line 80, in train_epoch
    optimizer.step()
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/optim/adam.py", line 234, in step
    adam(params_with_grad,
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/optim/adam.py", line 300, in adam
    func(params,
  File "/home/sssak/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/optim/adam.py", line 364, in _single_tensor_adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)
KeyboardInterrupt